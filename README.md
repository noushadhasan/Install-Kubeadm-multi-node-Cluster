# Kubernetes Cluster Setup with kubeadm

This guide explains how to set up a Kubernetes cluster from scratch using **kubeadm**, **containerd**, and **Flannel** as the CNI (Container Network Interface).

---

## üñ•Ô∏è Prerequisites

* Multiple Linux nodes (Ubuntu recommended)
* One control plane (master) node and one or more worker nodes
* `sudo` privileges on all nodes
* Internet access for downloading packages

---

## 1. Install Kubernetes Components (On Every Node)

```bash
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Install kubelet, kubeadm, and kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
kubeadm version
```

---

## 2. Install and Configure containerd (On Every Node)

```bash
sudo apt install containerd -y
sudo mkdir -p /etc/containerd

# Generate default config and enable systemd cgroup driver
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml

# Verify
cat /etc/containerd/config.toml | grep -i SystemdCgroup

# Restart containerd
sudo systemctl restart containerd
```

---

## 3. Enable IPv4 Packet Forwarding (On Every Node)

```bash
# Configure sysctl params
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply changes
sudo sysctl --system

# Verify
sysctl net.ipv4.ip_forward

# Restart kubelet
sudo systemctl restart kubelet
```

---

## 4. Configure Kubelet Cgroup Driver (On Every Node)

Check if the system is using `systemd`:

```bash
ps -p 1
```

If output is `systemd`, then kubeadm v1.22+ automatically defaults to the systemd cgroup driver ‚úÖ No need to do anything about it.

---

## 5. Initialize the Cluster (On Control Plane Node)

```bash
sudo kubeadm init \
  --apiserver-advertise-address <master_node_ip> \
  --pod-network-cidr "10.244.0.0/16" \
  --upload-certs
```

Set up kubeconfig:

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

---

## 6. Install Network Add-on (Flannel)

```bash
wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
kubectl apply -f kube-flannel.yml

# Verify flannel pods
kubectl get po -n kube-flannel
```

> ‚ö†Ô∏è Make sure the CIDR block in `kube-flannel.yml` matches `10.244.0.0/16`.

---

## 7. Join Worker Nodes (On Each Worker Node)

Run the join command generated by `kubeadm init`. Example:

```bash
kubeadm join 192.168.0.193:6443 --token 68bb5s.nz1n1wo2pteqef82 \
  --discovery-token-ca-cert-hash sha256:299767b1c4cc09463141dc8c14b8c7263012ec21bdbbc2b0524fd013ddc66e07
```

---

## 8. Label Worker Nodes (On Control Plane)

```bash
kubectl label node node01 node-role.kubernetes.io/worker=worker1
kubectl label node node02 node-role.kubernetes.io/worker=worker2

# Verify
kubectl get nodes -o wide
```

---

## ‚úÖ Done!

You now have a working Kubernetes cluster with containerd and Flannel networking.
